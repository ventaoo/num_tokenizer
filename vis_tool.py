"""
vis_tool.py => å¯è§†åŒ–ï¼Œä¿å­˜ç›¸å…³çš„å·¥å…·å‡½æ•°
"""
import os
import math
import glob
import torch
import matplotlib.pyplot as plt

class ModelCheckpointer:
    def __init__(self, output_dir, prefix="model", max_saved=5):
        self.output_dir = output_dir
        self.prefix = prefix
        self.max_saved = max_saved
        self.best_val_loss = float('inf')
        
    def save_checkpoint(self, model, optimizer, step, epoch, val_loss=None, is_best=False, scheduler=None):
        """
        [ä¿®æ”¹] å¢åŠ äº† optimizer å’Œ scheduler å‚æ•°
        """
        checkpoint_path = os.path.join(self.output_dir, f"{self.prefix}_step_{step}.pth")
        
        if val_loss is not None and val_loss < self.best_val_loss:
            self.best_val_loss = val_loss

        save_dict = {
            'step': step,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(), # [æ–°å¢] ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
            'val_loss': val_loss,
            'best_val_loss': self.best_val_loss, # [æ–°å¢] è®°å½•æˆªæ­¢ç›®å‰çš„å†å²æœ€ä½³ Loss (ç”¨äºæ¢å¤)
        }
        
        # [æ–°å¢] å¦‚æœæœ‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œä¹Ÿä¿å­˜
        if scheduler is not None:
            save_dict['scheduler_state_dict'] = scheduler.state_dict()
        
        torch.save(save_dict, checkpoint_path)
        
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}" + 
              (f" (val_loss: {val_loss:.4f})" if val_loss is not None else ""))
        
        if is_best and val_loss is not None:
            best_path = os.path.join(self.output_dir, f"{self.prefix}_best.pth")
            torch.save(save_dict, best_path)
            print(f"ğŸ‰ New best model updated => val_loss: {val_loss:.4f}")
        
        self._cleanup_old_checkpoints()

    def load_checkpoint(self, checkpoint_path, model, optimizer=None, scheduler=None):
        """
        [æ–°å¢] åŠ è½½æ£€æŸ¥ç‚¹ç”¨äºæ¢å¤è®­ç»ƒ
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
            
        print(f"æ­£åœ¨åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        # 1. æ¢å¤æ¨¡å‹æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # 2. æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ (å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼Œè¿™æ­¥å¾ˆå…³é”®)
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
        # 3. æ¢å¤è°ƒåº¦å™¨ (å¦‚æœæœ‰)
        if scheduler and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # 4. best loss
        saved_best = checkpoint.get('best_val_loss', float('inf'))
        if saved_best < self.best_val_loss:
            self.best_val_loss = saved_best
            
        start_epoch = checkpoint.get('epoch', 0)
        global_step = checkpoint.get('step', 0)
        
        print(f"æ¢å¤æˆåŠŸ! ä» Epoch {start_epoch}, Step {global_step} å¼€å§‹")
        return start_epoch, global_step
    
    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹"""
        checkpoint_files = glob.glob(os.path.join(self.output_dir, f"{self.prefix}_step_*.pth"))
        
        if len(checkpoint_files) <= self.max_saved:
            return
        
        checkpoint_files.sort(key=os.path.getmtime) 
        
        files_to_keep = checkpoint_files[-self.max_saved+1:]  # æœ€æ–°çš„ N-1 ä¸ª
        files_to_keep.append(checkpoint_files[0])             # ä¿ç•™æœ€è€çš„ä¸€ä¸ª (Start point)
        
        for f in checkpoint_files:
            if f not in files_to_keep:
                try:
                    os.remove(f)
                    print(f"æ¸…ç†æ—§æ£€æŸ¥ç‚¹: {os.path.basename(f)}")
                except OSError as e:
                    print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {f}: {e}")

def draw_metric_subplot(ax, train_x, val_x, config):
    train_data = config.get('train_data', [])
    val_data = config.get('val_data', [])
    
    # é¢œè‰²é»˜è®¤å€¼
    t_color = config.get('train_color', 'blue')
    v_color = config.get('val_color', 'orange')
    
    # ç»˜åˆ¶æ›²çº¿
    if train_data:
        ax.plot(train_x, train_data, '.-', label='Train', color=t_color, 
                markersize=5, linewidth=1.5)
    if val_data:
        ax.plot(val_x, val_data, '.-', label='Val', color=v_color, alpha=0.7,
                markersize=5, linewidth=1.5)

    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(config.get('title', ''), fontsize=12, fontweight='bold')
    ax.set_ylabel(config.get('ylabel', ''), fontsize=10)
    ax.set_xlabel("Global Steps", fontsize=10)
    
    # å›¾ä¾‹ä¸ç½‘æ ¼
    ax.legend(fontsize=9, loc='best')
    ax.grid(True, linestyle='--', alpha=0.3)

    # å¤„ç†å¯¹æ•°åæ ‡
    if config.get('use_log', False):
        ax.set_yscale('log')
    else:
        all_data = []
        if train_data: all_data.extend(train_data)
        if val_data: all_data.extend(val_data)
        
        if all_data:
            y_min, y_max = min(all_data), max(all_data)
            if y_min != y_max:
                margin = (y_max - y_min) * 0.05
                ax.set_ylim(y_min - margin, y_max + margin)

def plot_compare_curves(history, plot_configs, save_path=None, max_cols=4):
    """
    ä¸»ç»˜å›¾å‡½æ•°
    :param history: åŒ…å«è®­ç»ƒæ•°æ®çš„å­—å…¸
    :param plot_configs: é…ç½®åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    :param save_path: ä¿å­˜è·¯å¾„
    :param max_cols: ç½‘æ ¼å¸ƒå±€çš„æœ€å¤§åˆ—æ•°
    """
    train_x = history.get("train_steps", [])
    val_x = history.get("val_steps", [])

    # 1. å¦‚æœæ²¡æœ‰ä¼ å…¥é…ç½®ï¼Œåˆ™å®šä¹‰é»˜è®¤é…ç½® (å…¼å®¹æ—§ä»£ç é€»è¾‘)

    num_plots = len(plot_configs)
    if num_plots == 0:
        print("æ²¡æœ‰å¯ç»˜åˆ¶çš„æ•°æ®é…ç½®ã€‚")
        return

    # 2. è‡ªåŠ¨è®¡ç®—è¡Œæ•°å’Œåˆ—æ•°
    # å¦‚æœåªæœ‰1å¼ å›¾ï¼Œåˆ™ 1x1ï¼›å¦‚æœæœ‰3å¼ ä¸” max_cols=2ï¼Œåˆ™ 2x2
    ncols = min(num_plots, max_cols)
    nrows = math.ceil(num_plots / ncols)

    # 3. åˆå§‹åŒ–ç”»å¸ƒ
    # åŠ¨æ€è°ƒæ•´é«˜åº¦ï¼šæ¯è¡Œå¤§çº¦ç»™ 4-5 inches
    fig_width = 6 * ncols
    fig_height = 5 * nrows
    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height), sharex=False)
    
    # ç¡®ä¿ axes æ€»æ˜¯å¯è¿­ä»£çš„æ‰å¹³æ•°ç»„ï¼ˆå³ä½¿åªæœ‰1ä¸ªå­å›¾ï¼‰
    if num_plots == 1:
        axes = [axes]
    else:
        axes = axes.flatten()

    # 4. å¾ªç¯ç»˜åˆ¶
    for idx, config in enumerate(plot_configs):
        ax = axes[idx]
        draw_metric_subplot(ax, train_x, val_x, config)

    # 5. éšè—å¤šä½™çš„ç©ºåæ ‡è½´ (ä¾‹å¦‚ 2x2 ç½‘æ ¼åªæœ‰ 3 å¼ å›¾æ—¶)
    for i in range(num_plots, len(axes)):
        axes[i].axis('off')

    plt.tight_layout()
    
    # 6. ä¿å­˜é€»è¾‘
    if save_path:
        dir_name = os.path.dirname(save_path)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"å›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    plt.close(fig)